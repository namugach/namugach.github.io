---
tags:
  - python
  - kafka
  - mysql
  - pipeline
create: 2024-07-25 02:12:58
---
[[python 간단 파이프라인]]

물론, 카프카(Kafka)를 붙여서 데이터 파이프라인을 확장할 수 있어. Apache Kafka는 대규모 데이터 스트리밍을 처리하는 데 강력한 도구야. 카프카를 데이터 파이프라인에 통합하면 실시간 데이터 스트리밍과 처리를 더 유연하게 할 수 있어.

# 카프카를 데이터 파이프라인에 통합하기

1. **Kafka 설치 및 설정:**

   - **카프카 설치**: 카프카와 Zookeeper를 설치해야 해. [카프카 공식 문서](https://kafka.apache.org/quickstart)에서 설치 방법을 확인할 수 있어.
   - **카프카 브로커 실행**: 카프카 브로커와 Zookeeper를 시작해.

2. **카프카 Python 클라이언트 라이브러리 설치:**

   카프카와 상호작용할 파이썬 라이브러리인 `confluent-kafka`를 설치해야 해:

   ```sh
   pip install confluent-kafka
   ```

3. **데이터 생성 및 카프카에 데이터 전송:**

   CSV 파일의 데이터를 카프카 토픽으로 보내는 예제를 보여줄게:

   ```python
   from confluent_kafka import Producer
   import pandas as pd
   import json

   # 카프카 프로듀서 설정
   conf = {
       'bootstrap.servers': 'localhost:9092',  # 카프카 브로커 주소
   }
   producer = Producer(conf)

   # CSV 파일에서 데이터 추출
   data = pd.read_csv('data.csv')

   # 데이터를 카프카 토픽으로 전송
   def delivery_report(err, msg):
       if err is not None:
           print(f"Message delivery failed: {err}")
       else:
           print(f"Message delivered to {msg.topic()} [{msg.partition()}]")

   for index, row in data.iterrows():
       message = json.dumps({
           'name': row['name'],
           'age': row['age']
       })
       producer.produce('my_topic', key=str(index), value=message, callback=delivery_report)

   producer.flush()
   ```

4. **카프카에서 데이터를 읽어 MySQL에 적재하기:**

   카프카에서 데이터를 소비하고, MySQL 데이터베이스에 적재하는 스크립트야:

   ```python
   from confluent_kafka import Consumer
   import pymysql
   import json

   # MySQL 연결 설정
   connection = pymysql.connect(
       host='localhost',
       user='ubuntu',
       password='1234',
       database='testdb'
   )

   # 카프카 소비자 설정
   conf = {
       'bootstrap.servers': 'localhost:9092',
       'group.id': 'my_group',
       'auto.offset.reset': 'earliest'
   }
   consumer = Consumer(conf)
   consumer.subscribe(['my_topic'])

   try:
       with connection.cursor() as cursor:
           # 테이블 생성 (존재하지 않는 경우)
           create_table_query = """
           CREATE TABLE IF NOT EXISTS users (
               id INT AUTO_INCREMENT PRIMARY KEY,
               name VARCHAR(100),
               age INT
           )
           """
           cursor.execute(create_table_query)

           for message in consumer:
               data = json.loads(message.value().decode('utf-8'))
               insert_query = "INSERT INTO users (name, age) VALUES (%s, %s)"
               cursor.execute(insert_query, (data['name'], data['age']))
               connection.commit()
   finally:
       connection.close()
       consumer.close()
   ```

### 설명:

1. **데이터 전송 (카프카 프로듀서):**
   - CSV 파일에서 데이터를 읽고, 각 행을 JSON 형식으로 변환한 후 카프카 토픽(`my_topic`)으로 전송해.

2. **데이터 소비 및 MySQL 적재 (카프카 소비자):**
   - 카프카에서 메시지를 소비하고, JSON 형식의 데이터를 파싱하여 MySQL 데이터베이스에 적재해.

이렇게 하면 데이터 파이프라인이 확장되어 실시간으로 카프카에서 데이터를 처리하고, MySQL에 적재할 수 있어. 카프카를 추가함으로써 데이터 스트리밍, 처리, 그리고 저장을 훨씬 유연하고 강력하게 할 수 있어.

---

## 기록 조회하기

위 코드에서 MySQL 데이터베이스와 Kafka를 이용하여 데이터를 저장하고 전송하는 작업을 수행하고 있습니다. 각 부분의 작업 기록을 확인할 수 있는 방법은 다음과 같습니다:

1. **MySQL 작업 기록**:
	- **데이터베이스와 테이블 생성**: `my_data`라는 데이터베이스와 `users` 테이블이 생성되었는지 확인하려면 MySQL에 접속하여 `SHOW DATABASES;`와 `SHOW TABLES IN my_data;` 명령을 실행하면 됩니다.
	- **데이터 삽입 기록**: `users` 테이블에 삽입된 데이터는 `SELECT * FROM users;` 쿼리를 실행하여 확인할 수 있습니다. 이는 코드 내에서도 확인하고 있습니다.

2. **Kafka 메시지 기록**:
	- Kafka는 주로 로그 파일이나 Kafka 커맨드 라인을 통해 메시지의 기록을 확인할 수 있습니다. Kafka의 `kafka-console-consumer` 유틸리티를 사용하여 특정 토픽(`my_topic`)의 메시지를 소비하면서 실시간으로 전송된 데이터를 확인할 수 있습니다.

	예를 들어, 터미널에서 다음과 같은 명령을 사용할 수 있습니다:

```bash
kafka-console-consumer --bootstrap-server localhost:9092 --topic my_topic --from-beginning
```

이 명령은 `my_topic`에 게시된 모든 메시지를 표시합니다. 이를 통해 어떤 데이터가 전송되었는지 확인할 수 있습니다.
