---
tags:
  - kafka
  - python
  - mysql
create: 2024-07-27 14:52:13
---

Kafka를 사용했을 때와 그렇지 않았을 때의 차이를 극명하게 느끼려면, Kafka의 핵심 기능인 **실시간 데이터 스트리밍**과 **확장성**의 이점을 실제로 체험해보는 것이 중요합니다. 이를 위해 다음과 같은 시나리오를 비교해 볼 수 있습니다:

## 카프카를 사용하는 이유와 체감

### 시나리오 1: Kafka 없이 MySQL로 직접 데이터 저장
이 경우, 데이터는 MySQL 데이터베이스에 직접 삽입되고, 데이터베이스에서 실시간 데이터 분석이나 다른 시스템으로의 데이터 전송은 별도의 작업으로 처리됩니다.

#### 장점:
- 시스템이 단순해지고, 데이터베이스 관리와 유지보수가 쉬워집니다.

#### 단점:
- 실시간 데이터 처리가 어렵고, 데이터 처리 속도가 데이터베이스 성능에 제한됩니다.
- 확장성이 낮아, 높은 데이터 트래픽이나 다양한 데이터 소비 요구에 대응하기 어렵습니다.
- 여러 시스템 간 데이터 통합이나 데이터 동기화가 복잡해질 수 있습니다.

### 시나리오 2: Kafka 사용
Kafka를 사용하면 데이터는 Kafka 토픽에 게시되고, MySQL 데이터베이스는 Kafka의 컨슈머 중 하나로서 데이터를 소비하게 됩니다. 이 경우 Kafka는 데이터 브로커로서 다양한 시스템에 데이터를 스트리밍합니다.

#### 장점:
- **실시간 데이터 스트리밍**: Kafka는 데이터를 실시간으로 여러 컨슈머에게 스트리밍할 수 있어, 데이터베이스 저장 외에도 실시간 분석, 모니터링, 알림 시스템 등 다양한 사용 사례에 데이터를 즉시 사용할 수 있습니다.
- **확장성**: Kafka 클러스터를 확장하여 더 많은 데이터를 처리할 수 있으며, 새로운 컨슈머를 쉽게 추가하여 다양한 애플리케이션이 데이터를 소비할 수 있습니다.
- **내구성**: Kafka는 데이터를 디스크에 저장하고 복제하므로, 시스템 장애 시에도 데이터 손실이 최소화됩니다.
- **분리된 시스템**: 데이터 생산자와 소비자 시스템 간의 결합이 느슨해져, 각각 독립적으로 확장하거나 업데이트할 수 있습니다.

#### 단점:
- 시스템 복잡도가 증가하고, Kafka 클러스터 운영과 관리에 추가적인 노력이 필요합니다.
- 초기 설정 및 학습 곡선이 있습니다.

### 실험을 통한 차이 체험

실제로 Kafka를 사용했을 때와 사용하지 않았을 때의 차이를 체험하려면 다음과 같은 실험을 해볼 수 있습니다:

1. **고속 데이터 인제스트 시나리오**:
   - 대량의 데이터를 짧은 시간에 인제스트할 때, Kafka가 이를 효과적으로 버퍼링하고 처리하는 반면, MySQL은 성능 병목 현상이 발생할 수 있습니다.

2. **실시간 분석 시나리오**:
   - Kafka를 사용하여 데이터가 발생하는 즉시 스트리밍 데이터를 분석하는 컨슈머를 연결하고, MySQL에서는 데이터베이스 쿼리로 실시간 데이터를 분석해보십시오. Kafka의 스트리밍 처리가 얼마나 신속하고 효율적인지 알 수 있습니다.

3. **다양한 소비자 연결 시나리오**:
   - 같은 데이터를 여러 시스템이 소비해야 하는 경우, Kafka의 pub-sub 모델을 사용하여 이를 간단하게 구현할 수 있습니다. MySQL을 사용하면 데이터베이스에서 직접 데이터 복사를 설정하거나 별도의 통합 솔루션이 필요할 수 있습니다.

이러한 실험을 통해 Kafka의 장점을 직접 경험할 수 있으며, 이를 통해 Kafka 사용의 실질적인 이점을 명확하게 느낄 수 있습니다.

---

## 실습

CSV 데이터를 사용하여 Kafka를 통해 데이터를 스트리밍하고, MySQL에 저장하는 예제를 제공하겠습니다. 이 예제에서는 CSV 파일에서 데이터를 읽어 Kafka에 게시하고, Kafka 컨슈머가 이 데이터를 소비하여 MySQL에 저장하는 방식으로 진행됩니다.

### 사전 준비
- CSV 파일: 예시로 사용될 데이터를 포함한 파일. 예를 들어, `users.csv`라는 파일을 사용할 수 있습니다.
- MySQL 데이터베이스 서버
- Kafka 브로커 및 Zookeeper
- Kafka Python 클라이언트 (`kafka-python` 패키지)
- MySQL Python 클라이언트 (`pymysql` 패키지)
- Pandas (CSV 데이터를 읽기 위해 사용)

### CSV 파일 예시 (`users.csv`)
```
name,age
Alice,30
Bob,24
Charlie,29
```

### 설정 및 데이터베이스 테이블 생성
먼저, MySQL에 `users` 테이블을 생성합니다.

```python
import pymysql

# MySQL 연결 설정
connection = pymysql.connect(
    host='localhost',
    user='root',
    password='password',
    database='my_data'
)

# 테이블 생성 쿼리
create_table_query = """
CREATE TABLE IF NOT EXISTS users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100),
    age INT
)
"""

try:
    with connection.cursor() as cursor:
        cursor.execute(create_table_query)
    connection.commit()
finally:
    connection.close()
```

### 시나리오 1: MySQL로 직접 CSV 데이터 저장
CSV 파일의 데이터를 읽어 MySQL에 직접 삽입하는 코드입니다.

```python
import pandas as pd
import pymysql

def insert_csv_into_mysql(file_path):
    # CSV 데이터 읽기
    data = pd.read_csv(file_path)

    # MySQL 연결 설정
    connection = pymysql.connect(
        host='localhost',
        user='root',
        password='password',
        database='my_data'
    )

    insert_query = "INSERT INTO users (name, age) VALUES (%s, %s)"
    try:
        with connection.cursor() as cursor:
            # CSV 데이터를 MySQL에 삽입
            for _, row in data.iterrows():
                cursor.execute(insert_query, (row['name'], row['age']))
        connection.commit()
    finally:
        connection.close()

# CSV 파일 경로
csv_file_path = 'users.csv'

insert_csv_into_mysql(csv_file_path)
print("MySQL에 CSV 데이터 삽입 완료.")
```

### 시나리오 2: Kafka 사용

#### Kafka Producer (CSV 데이터를 Kafka에 게시)
CSV 파일의 데이터를 Kafka 토픽에 게시하는 코드입니다.

```python
from kafka import KafkaProducer
import pandas as pd
import json

def produce_csv_to_kafka(file_path, topic):
    # CSV 데이터 읽기
    data = pd.read_csv(file_path)

    producer = KafkaProducer(
        bootstrap_servers='localhost:9092',
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

    # 각 행을 Kafka 메시지로 게시
    for _, row in data.iterrows():
        message = row.to_dict()
        producer.send(topic, value=message)

    producer.flush()
    print("Kafka에 CSV 데이터 게시 완료.")

produce_csv_to_kafka(csv_file_path, 'my_topic')
```

#### Kafka Consumer (Kafka에서 데이터를 소비하여 MySQL에 저장)
Kafka에서 데이터를 소비하고 MySQL에 저장하는 코드입니다.

```python
from kafka import KafkaConsumer
import pymysql
import json

def consume_from_kafka_and_insert_into_mysql(topic):
    consumer = KafkaConsumer(
        topic,
        bootstrap_servers='localhost:9092',
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    def insert_into_mysql(data):
        connection = pymysql.connect(
            host='localhost',
            user='root',
            password='password',
            database='my_data'
        )

        insert_query = "INSERT INTO users (name, age) VALUES (%s, %s)"
        try:
            with connection.cursor() as cursor:
                cursor.executemany(insert_query, [(d['name'], d['age']) for d in data])
            connection.commit()
        finally:
            connection.close()

    buffer = []
    for message in consumer:
        data = message.value
        buffer.append(data)
        if len(buffer) >= 10:  # 예시로 10개씩 버퍼링하여 MySQL에 삽입
            insert_into_mysql(buffer)
            buffer = []

    # 남은 데이터 삽입
    if buffer:
        insert_into_mysql(buffer)

    print("Kafka에서 데이터 소비 및 MySQL에 저장 완료.")

consume_from_kafka_and_insert_into_mysql('my_topic')
```

### 실습 방법

1. **시나리오 1**:
   - `insert_csv_into_mysql` 함수를 호출하여 CSV 데이터를 MySQL에 직접 삽입합니다.
   
2. **시나리오 2**:
   - `produce_csv_to_kafka` 함수를 호출하여 CSV 데이터를 Kafka에 게시한 후, `consume_from_kafka_and_insert_into_mysql` 함수를 호출하여 Kafka에서 데이터를 소비하고 MySQL에 저장합니다.

이 실습을 통해 CSV 데이터를 사용하여 Kafka의 사용 여부에 따른 데이터 처리 방식을 비교하고, Kafka의 장점(실시간 처리, 확장성 등)을 직접 체험할 수 있습니다.